{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Good job 100/100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Adult income Data Prediction\n",
    "#### Group members: \n",
    ">Yingxue Zhu (820894341)  Xuan Li (819264580)\n",
    "              \n",
    "#### Goal of the project: \n",
    ">Determine whether a person makes over 50K a year.\n",
    "\n",
    "#### Dataset reference: \n",
    ">https://archive.ics.uci.edu/ml/datasets/adult \n",
    "              \n",
    ">This data was extracted from the census bureau database found at http://www.census.gov/ftp/pub/DES/www/welcome.html\n",
    "\n",
    ">Donor: Ronny Kohavi and Barry Becker\n",
    "\n",
    ">Split into train-test using MLC++ GenCVFiles (2/3, 1/3 random).\n",
    "\n",
    "#### Data description\n",
    ">http://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.names\n",
    "\n",
    "#### Files and kernels\n",
    ">income_data_analysis.ipynb -- please use kernel Apache Toree - Scala\n",
    "\n",
    ">data_visualization.ipynb   -- please use kernel Python 3\n",
    "\n",
    "\n",
    "#### Organization\n",
    "\n",
    ">income_data_analysis.ipynb\n",
    "\n",
    "> * 1 Load data\n",
    "* 2 Rename \n",
    "* 3 Data cleaning & Label encode income column\n",
    "* 4 Explore data\n",
    "   * 4.1 Probability of income >50k and <=50k\n",
    "   * 4.2 Influence of different features on income\n",
    "   * 4.3 Data Visualization\n",
    "* 5 Transform text type columns\n",
    "* 6 Pearson Correlation\n",
    "* 7 Assemble raw features\n",
    "* 8 Prediction:\n",
    "   * 8.1 Logistic Regression\n",
    "   * 8.2 Decision Tree\n",
    "   * 8.3 Random Forest\n",
    "   * 8.4 Naive Bayes\n",
    "   * 8.5 Neural Network\n",
    "   * 8.6 Gradient-Boosted Trees Classifier\n",
    "* 9 Conclusion\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import org.apache.spark.sql.functions._\n",
    "import org.apache.spark.ml.feature.StringIndexer\n",
    "import org.apache.spark.sql.SparkSession\n",
    "import org.apache.spark.SparkConf\n",
    "import org.apache.spark.SparkContext\n",
    "import org.apache.spark.ml.feature.RFormula\n",
    "import org.apache.spark.ml.classification.{RandomForestClassifier}\n",
    "import org.apache.spark.ml.classification.{LogisticRegression, LogisticRegressionModel}\n",
    "import org.apache.spark.ml.{Pipeline}\n",
    "import org.apache.spark.mllib.linalg.{Matrix, Matrices}\n",
    "import org.apache.spark.ml.feature.VectorAssembler\n",
    "import org.apache.spark.ml.evaluation.MulticlassClassificationEvaluator\n",
    "import org.apache.spark.mllib.evaluation.MulticlassMetrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Load training data and testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----------------+-------+----------+----+-------------------+----------------+--------------+------+-----+------+----+----+--------------+------+\n",
      "|_c0|              _c1|    _c2|       _c3| _c4|                _c5|             _c6|           _c7|   _c8|  _c9|  _c10|_c11|_c12|          _c13|  _c14|\n",
      "+---+-----------------+-------+----------+----+-------------------+----------------+--------------+------+-----+------+----+----+--------------+------+\n",
      "| 39|        State-gov|77516.0| Bachelors|13.0|      Never-married|    Adm-clerical| Not-in-family| White| Male|2174.0| 0.0|40.0| United-States| <=50K|\n",
      "| 28| Self-emp-not-inc|83311.0| Bachelors|13.0| Married-civ-spouse| Exec-managerial|       Husband| White| Male|   0.0| 0.0|13.0| United-States| <=50K|\n",
      "+---+-----------------+-------+----------+----+-------------------+----------------+--------------+------+-----+------+----+----+--------------+------+\n",
      "only showing top 2 rows\n",
      "\n",
      "Total sample size of train data: 32561\n",
      "+---+--------+--------+--------+---+-------------------+------------------+----------+------+-----+----+----+----+--------------+-------+\n",
      "|_c0|     _c1|     _c2|     _c3|_c4|                _c5|               _c6|       _c7|   _c8|  _c9|_c10|_c11|_c12|          _c13|   _c14|\n",
      "+---+--------+--------+--------+---+-------------------+------------------+----------+------+-----+----+----+----+--------------+-------+\n",
      "| 25| Private|226802.0|    11th|7.0|      Never-married| Machine-op-inspct| Own-child| Black| Male| 0.0| 0.0|40.0| United-States| <=50K.|\n",
      "| 38| Private| 89814.0| HS-grad|9.0| Married-civ-spouse|   Farming-fishing|   Husband| White| Male| 0.0| 0.0|50.0| United-States| <=50K.|\n",
      "+---+--------+--------+--------+---+-------------------+------------------+----------+------+-----+----+----+----+--------------+-------+\n",
      "only showing top 2 rows\n",
      "\n",
      "Total sample size of test data: 16281\n",
      "Total instances of data: 48842\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "reader = org.apache.spark.sql.DataFrameReader@7c5ea4ed\n",
       "currentDirectory = /Users/xuanli/Downloads\n",
       "trainDataDir = /Users/xuanli/Downloads/adult.data\n",
       "trainData = [_c0: int, _c1: string ... 13 more fields]\n",
       "train_size = 32561\n",
       "testDataDir = /Users/xuanli/Downloads/adult.test\n",
       "testData = [_c0: int, _c1: string ... 13 more fields]\n",
       "test_size = 16281\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "16281"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//load training data\n",
    "val reader = spark.read\n",
    "var currentDirectory = new java.io.File(\".\").getCanonicalPath\n",
    "val trainDataDir = currentDirectory + \"/adult.data\"\n",
    "val trainData = reader.format(\"csv\").\n",
    "      option(\"mode\",\"DROPMALFORMED\").\n",
    "      option(\"nullValue\", \" ?\").\n",
    "      option(\"header\",false).\n",
    "      option(\"inferSchema\",true).\n",
    "      option(\"sep\", \",\").\n",
    "      load(trainDataDir)\n",
    "trainData.show(2)\n",
    "var train_size = trainData.count()\n",
    "println(\"Total sample size of train data: \" + train_size)\n",
    "//load testing data\n",
    "val testDataDir = currentDirectory + \"/adult.test\"\n",
    "val testData = reader.format(\"csv\").\n",
    "      option(\"mode\",\"DROPMALFORMED\").\n",
    "      option(\"nullValue\", \" ?\").\n",
    "      option(\"header\",false).\n",
    "      option(\"inferSchema\",true).\n",
    "      option(\"sep\", \",\").\n",
    "      load(testDataDir)\n",
    "testData.show(2)\n",
    "var test_size = testData.count()\n",
    "println(\"Total sample size of test data: \" + test_size)\n",
    "println(\"Total instances of data: \" + (train_size + test_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Before remove of those unknown:<br/>\n",
    "Total sample size of train data: 32561<br/>\n",
    "Total sample size of test data: 16281<br/>\n",
    "Total instances: 48842<br/>\n",
    "which are same numbers in data description. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Rename "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----------------+--------+----------+-------------+-------------------+------------------+--------------+------+-------+------------+------------+--------------+--------------+------+\n",
      "|age|        workclass|  fnlwgt| education|education_num|     marital_status|        occupation|  relationship|  race|    sex|capital_gain|capital_loss|hours_per_week|native_country|income|\n",
      "+---+-----------------+--------+----------+-------------+-------------------+------------------+--------------+------+-------+------------+------------+--------------+--------------+------+\n",
      "| 39|        State-gov| 77516.0| Bachelors|         13.0|      Never-married|      Adm-clerical| Not-in-family| White|   Male|      2174.0|         0.0|          40.0| United-States| <=50K|\n",
      "| 28| Self-emp-not-inc| 83311.0| Bachelors|         13.0| Married-civ-spouse|   Exec-managerial|       Husband| White|   Male|         0.0|         0.0|          13.0| United-States| <=50K|\n",
      "| 38|          Private|215646.0|   HS-grad|          9.0|           Divorced| Handlers-cleaners| Not-in-family| White|   Male|         0.0|         0.0|          40.0| United-States| <=50K|\n",
      "| 53|          Private|234721.0|      11th|          7.0| Married-civ-spouse| Handlers-cleaners|       Husband| Black|   Male|         0.0|         0.0|          40.0| United-States| <=50K|\n",
      "| 28|          Private|338409.0| Bachelors|         13.0| Married-civ-spouse|    Prof-specialty|          Wife| Black| Female|         0.0|         0.0|          40.0|          Cuba| <=50K|\n",
      "+---+-----------------+--------+----------+-------------+-------------------+------------------+--------------+------+-------+------------+------------+--------------+--------------+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "newNames = List(age, workclass, fnlwgt, education, education_num, marital_status, occupation, relationship, race, sex, capital_gain, capital_loss, hours_per_week, native_country, income)\n",
       "trainDataRenamed = [age: int, workclass: string ... 13 more fields]\n",
       "testDataRenamed = [age: int, workclass: string ... 13 more fields]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[age: int, workclass: string ... 13 more fields]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val newNames = Seq(\"age\", \"workclass\", \"fnlwgt\", \"education\", \"education_num\", \"marital_status\", \"occupation\", \"relationship\", \"race\", \"sex\", \"capital_gain\", \"capital_loss\", \"hours_per_week\", \"native_country\", \"income\")\n",
    "val trainDataRenamed = trainData.toDF(newNames: _*)\n",
    "val testDataRenamed = testData.toDF(newNames: _*)\n",
    "trainDataRenamed.show(5)\n",
    "//testDataRenamed.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Data cleaning and Label encode income column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total number of entries: 45222\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "indexerTrain = strIdx_0b127e504d1f\n",
       "trainDataRemoved = [age: int, workclass: string ... 14 more fields]\n",
       "indexerTest = strIdx_d01baa48ae81\n",
       "testDataRemoved = [age: int, workclass: string ... 14 more fields]\n",
       "numTrain = 30162\n",
       "numTest = 15060\n",
       "total = 45222\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "45222"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//label encoder for income column \n",
    "val indexerTrain = new StringIndexer().setInputCol(\"income\")\n",
    "                                 .setOutputCol(\"label\")\n",
    "                                 .fit(trainDataRenamed)\n",
    "val trainDataRemoved = indexerTrain.transform(trainDataRenamed).na.drop()\n",
    "val indexerTest = new StringIndexer().setInputCol(\"income\")\n",
    "                                 .setOutputCol(\"label\")\n",
    "                                 .fit(testDataRenamed)\n",
    "val testDataRemoved = indexerTest.transform(testDataRenamed).na.drop()\n",
    "//trainDataRemoved.printSchema\n",
    "//testDataRemoved.printSchema\n",
    "val numTrain = trainDataRemoved.count()\n",
    "val numTest = testDataRemoved.count()\n",
    "val total = numTrain + numTest\n",
    "println(\"total number of entries: \" + total)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After remove missing value:<br/>\n",
    "Total sample size of train data: 30162<br/>\n",
    "Total sample size of test data: 15060<br/>\n",
    "Total instances: 45222<br/>\n",
    "which are same numbers in data description. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Explore data\n",
    "## 4.1 percentage of income >50k and <=50k "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----------------+-------+----------+-------------+-------------------+----------------+--------------+------+-----+------------+------------+--------------+--------------+------+-----+\n",
      "|age|        workclass| fnlwgt| education|education_num|     marital_status|      occupation|  relationship|  race|  sex|capital_gain|capital_loss|hours_per_week|native_country|income|label|\n",
      "+---+-----------------+-------+----------+-------------+-------------------+----------------+--------------+------+-----+------------+------------+--------------+--------------+------+-----+\n",
      "| 39|        State-gov|77516.0| Bachelors|         13.0|      Never-married|    Adm-clerical| Not-in-family| White| Male|      2174.0|         0.0|          40.0| United-States| <=50K|  0.0|\n",
      "| 50| Self-emp-not-inc|83311.0| Bachelors|         13.0| Married-civ-spouse| Exec-managerial|       Husband| White| Male|         0.0|         0.0|          13.0| United-States| <=50K|  0.0|\n",
      "+---+-----------------+-------+----------+-------------+-------------------+----------------+--------------+------+-----+------------+------------+--------------+--------------+------+-----+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "//combine train and test \n",
    "val adultAll = trainDataRemoved.unionAll(testDataRemoved)\n",
    "adultAll.show(2)\n",
    "adultAll.count()\n",
    "val above = adultAll.filter(expr(\"label = 1\")).count().toDouble/total.toDouble\n",
    "val below = adultAll.filter(expr(\"label = 0\")).count().toDouble/total.toDouble"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 Influence of different features on income"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r",
      "+----------+-----------+\n",
      "|age_bucket|total_count|\n",
      "+----------+-----------+\n",
      "|   [30~35)|       6198|\n",
      "|   [35~40)|       6164|\n",
      "|   [25~30)|       5737|\n",
      "|   [40~45)|       5531|\n",
      "|   [20~25)|       5256|\n",
      "|   [45~50)|       4774|\n",
      "|   [50~55)|       3637|\n",
      "|   [55~60)|       2627|\n",
      "|   [15~20)|       2052|\n",
      "|   [60~65)|       1685|\n",
      "|   [65~70)|        829|\n",
      "|   [70~75)|        410|\n",
      "|   [75~80)|        179|\n",
      "|   [80~85)|         84|\n",
      "|   [90~95)|         46|\n",
      "|   [85~90)|         13|\n",
      "+----------+-----------+\n"
     ]
    }
   ],
   "source": [
    "//age category\n",
    "def age_range(x:Double):String = { \n",
    "    var n = x.toInt/5\n",
    "    var s =(n*5).toString\n",
    "    var e =(n*5+5).toString\n",
    "    \"[\"+s+\"~\"+e+\")\"\n",
    "}\n",
    "val age_type = udf(age_range(_:Double):String)\n",
    "val bucketed = adultAll.withColumn(\"age_bucket\", age_type(col(\"age\")))\n",
    "bucketed.groupBy(\"age_bucket\").agg(count(\"*\").alias(\"total_count\")).orderBy(desc(\"total_count\")).show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+------------------+\n",
      "|    education|  more than 50K(%)|\n",
      "+-------------+------------------+\n",
      "|  Prof-school| 75.41401273885351|\n",
      "|    Doctorate| 73.34558823529412|\n",
      "|      Masters| 55.40970564836913|\n",
      "|    Bachelors|41.981505944517835|\n",
      "|   Assoc-acdm|26.410086264100862|\n",
      "|    Assoc-voc|25.727411944869832|\n",
      "| Some-college| 20.10304071118295|\n",
      "|      HS-grad|16.343096800378813|\n",
      "|         12th| 7.452339688041594|\n",
      "|         10th| 6.704824202780049|\n",
      "|      7th-8th| 6.682867557715674|\n",
      "|          9th| 5.621301775147929|\n",
      "|         11th| 5.497220506485485|\n",
      "|      5th-6th|4.8997772828507795|\n",
      "|      1st-4th|3.6036036036036037|\n",
      "|    Preschool|1.3888888888888888|\n",
      "+-------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "//education vs income\n",
    "adultAll.groupBy(\"education\").\n",
    "    agg((sum(\"label\")/count(\"*\")*100).alias(\"more than 50K(%)\")).orderBy(desc(\"more than 50K(%)\")).show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Conclusion:**\n",
    "education has dramatic influence on income. \n",
    "Basically, the higher the person was educated, the higher chance the person has annual income greater than 50k."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+------------------+                                             \n",
      "|native_country|  more than 50K(%)|\n",
      "+--------------+------------------+\n",
      "|        Taiwan| 45.45454545454545|\n",
      "|        France| 44.44444444444444|\n",
      "|         India| 42.17687074829932|\n",
      "|       England| 39.49579831932773|\n",
      "|          Iran|39.285714285714285|\n",
      "|        Canada|36.809815950920246|\n",
      "|        Greece|36.734693877551024|\n",
      "|         Japan|34.831460674157306|\n",
      "|    Yugoslavia| 34.78260869565217|\n",
      "|      Cambodia| 34.61538461538461|\n",
      "|       Hungary| 33.33333333333333|\n",
      "|         Italy|              33.0|\n",
      "|         China|31.858407079646017|\n",
      "|       Germany| 30.05181347150259|\n",
      "|   Philippines|29.681978798586574|\n",
      "|          Hong| 28.57142857142857|\n",
      "|       Ireland| 27.77777777777778|\n",
      "|          Cuba|25.563909774436087|\n",
      "| United-States|25.302722076915625|\n",
      "|        Poland|19.753086419753085|\n",
      "+--------------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "// native_country vs income\n",
    "adultAll.groupBy(\"native_country\").\n",
    "    agg((sum(\"label\")/count(\"*\")*100).alias(\"more than 50K(%)\")).orderBy(desc(\"more than 50K(%)\")).show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The native country also has influence on income. The data shows that person from Taiwan has the highest chance to get annual income greater than 50k.\n",
    "And the lowest is from Poland."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+------------------+                                        \n",
      "|               race|  more than 50K(%)|\n",
      "+-------------------+------------------+\n",
      "| Asian-Pac-Islander| 28.31926323867997|\n",
      "|              White| 26.23705112716243|\n",
      "|              Other|12.747875354107649|\n",
      "|              Black|12.630085146641438|\n",
      "| Amer-Indian-Eskimo|12.183908045977011|\n",
      "+-------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "//race vs income\n",
    "adultAll.groupBy(\"race\").\n",
    "    agg((sum(\"label\")/count(\"*\")*100).alias(\"more than 50K(%)\")).orderBy(desc(\"more than 50K(%)\")).show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "The data shows race does not significant effect on income."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+                                                    \n",
      "|    sex|  more than 50K(%)|\n",
      "+-------+------------------+\n",
      "|   Male|31.247747895305793|\n",
      "| Female|11.357604627424294|\n",
      "+-------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "//sex vs income\n",
    "adultAll.groupBy(\"sex\").\n",
    "    agg((sum(\"label\")/count(\"*\")*100).alias(\"more than 50K(%)\")).orderBy(desc(\"more than 50K(%)\")).show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data shows that man that has annual income greater than 50k is about as twice as woman does."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------------------+                                       \n",
      "|      marital_status|  more than 50K(%)|\n",
      "+--------------------+------------------+\n",
      "|  Married-civ-spouse|45.423889812396105|\n",
      "|   Married-AF-spouse|             43.75|\n",
      "|            Divorced| 10.40177862474194|\n",
      "| Married-spouse-a...| 9.782608695652174|\n",
      "|             Widowed| 9.475332811276429|\n",
      "|           Separated| 7.016300496102056|\n",
      "|       Never-married| 4.802027675023976|\n",
      "+--------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "//marital_status vs income\n",
    "adultAll.groupBy(\"marital_status\").\n",
    "    agg((sum(\"label\")/count(\"*\")*100).alias(\"more than 50K(%)\")).orderBy(desc(\"more than 50K(%)\")).show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data shows marriage status does have influence on annual income. (Married spouse tend to get annual income over 50k)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.3 Data Visualization \n",
    "> see dataVisualization.ipynb\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. transform text type columns "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- age: integer (nullable = true)\n",
      " |-- workclass: string (nullable = true)\n",
      " |-- fnlwgt: double (nullable = true)\n",
      " |-- education: string (nullable = true)\n",
      " |-- education_num: double (nullable = true)\n",
      " |-- marital_status: string (nullable = true)\n",
      " |-- occupation: string (nullable = true)\n",
      " |-- relationship: string (nullable = true)\n",
      " |-- race: string (nullable = true)\n",
      " |-- sex: string (nullable = true)\n",
      " |-- capital_gain: double (nullable = true)\n",
      " |-- capital_loss: double (nullable = true)\n",
      " |-- hours_per_week: double (nullable = true)\n",
      " |-- native_country: string (nullable = true)\n",
      " |-- income: string (nullable = true)\n",
      " |-- label: double (nullable = true)\n",
      " |-- idx_workclass: double (nullable = true)\n",
      " |-- idx_education: double (nullable = true)\n",
      " |-- idx_marital_status: double (nullable = true)\n",
      " |-- idx_occupation: double (nullable = true)\n",
      " |-- idx_relationship: double (nullable = true)\n",
      " |-- idx_race: double (nullable = true)\n",
      " |-- idx_sex: double (nullable = true)\n",
      " |-- idx_native_country: double (nullable = true)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "stringColumns = Array(workclass, education, marital_status, occupation, relationship, race, sex, native_country)\n",
       "index_transformers = Array(strIdx_4fd6a6daf7b6, strIdx_f309bbc97292, strIdx_c1a8cacd6f33, strIdx_e9416b4b7cdd, strIdx_b35a2365e101, strIdx_dd424b002974, strIdx_8de12a432028, strIdx_9560abbfaa00)\n",
       "index_pipeline = pipeline_d96013e685d7\n",
       "index_model_train = pipeline_d96013e685d7\n",
       "trainData_tran = [age: int, workclass: string ... 22 more fields]\n",
       "index_model_test = pipeline_d96013e685d7\n",
       "testData_tran = [age: int, workclass: string ... 22 more fields]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[age: int, workclass: string ... 22 more fields]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//string index all string columns for both train and test data\n",
    "val stringColumns = Array(\"workclass\",\"education\",\"marital_status\",\n",
    "    \"occupation\",\"relationship\",\"race\",\"sex\",\n",
    "              \"native_country\")\n",
    "val index_transformers: Array[org.apache.spark.ml.PipelineStage] = stringColumns.map(\n",
    "      cname => new StringIndexer()\n",
    "        .setInputCol(cname)\n",
    "        .setOutputCol(s\"idx_${cname}\")\n",
    "    )\n",
    "val index_pipeline = new Pipeline().setStages(index_transformers)\n",
    "//train\n",
    "var index_model_train = index_pipeline.fit(trainDataRemoved)\n",
    "val trainData_tran = index_model_train.transform(trainDataRemoved)\n",
    "//test\n",
    "var index_model_test = index_pipeline.fit(testDataRemoved)\n",
    "val testData_tran = index_model_train.transform(testDataRemoved)\n",
    "trainData_tran.printSchema\n",
    "//testData_tran.printSchema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+-------------+------------+------------+--------------+-------------+-------------+------------------+--------------+----------------+--------+-------+------------------+-----+\n",
      "|age| fnlwgt|education_num|capital_gain|capital_loss|hours_per_week|idx_workclass|idx_education|idx_marital_status|idx_occupation|idx_relationship|idx_race|idx_sex|idx_native_country|label|\n",
      "+---+-------+-------------+------------+------------+--------------+-------------+-------------+------------------+--------------+----------------+--------+-------+------------------+-----+\n",
      "| 39|77516.0|         13.0|      2174.0|         0.0|          40.0|          3.0|          2.0|               1.0|           3.0|             1.0|     0.0|    0.0|               0.0|  0.0|\n",
      "| 28|83311.0|         13.0|         0.0|         0.0|          13.0|          1.0|          2.0|               0.0|           2.0|             0.0|     0.0|    0.0|               0.0|  0.0|\n",
      "+---+-------+-------------+------------+------------+--------------+-------------+-------------+------------------+--------------+----------------+--------+-------+------------------+-----+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "fields_train = Array(workclass, education, marital_status, occupation, relationship, race, sex, native_country, income)\n",
       "trainData_newDf = [age: int, fnlwgt: double ... 13 more fields]\n",
       "trainData_newDf = [age: int, fnlwgt: double ... 13 more fields]\n",
       "fields_test = Array(workclass, education, marital_status, occupation, relationship, race, sex, native_country, income)\n",
       "testData_newDf = [age: int, fnlwgt: double ... 13 more fields]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[age: int, fnlwgt: double ... 13 more fields]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//drop all the original text columns\n",
    "val fields_train = trainData_tran.schema.fields filter {\n",
    "x => x.dataType match { \n",
    "      case x: org.apache.spark.sql.types.StringType => true\n",
    "      case _ => false \n",
    "      } \n",
    "    } map { x => x.name }\n",
    "var trainData_newDf = fields_train.foldLeft(trainData_tran){ case(dframe,field) => dframe.drop(field) }\n",
    "trainData_newDf = trainData_newDf.withColumn(\"label2\", expr(\"label\")).drop(\"label\").withColumnRenamed(\"label2\",\"label\")\n",
    "//test\n",
    "val fields_test = testData_tran.schema.fields filter {\n",
    "x => x.dataType match { \n",
    "      case x: org.apache.spark.sql.types.StringType => true\n",
    "      case _ => false \n",
    "      } \n",
    "    } map { x => x.name }\n",
    "val testData_newDf = fields_test.foldLeft(testData_tran){ case(dframe,field) => dframe.drop(field) }\n",
    "trainData_newDf.show(2)\n",
    "//testData_newDf.show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Pearson Correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The correlation between age and income is: 0.24203068168136402\n",
      "The correlation between fnlwgt and income is: -0.008957423359171636\n",
      "The correlation between education_num and income is: 0.335286196752636\n",
      "The correlation between capital_gain and income is: 0.221196214548056\n",
      "The correlation between capital_loss and income is: 0.1500533083972996\n",
      "The correlation between hours_per_week and income is: 0.22948012988851152\n",
      "The correlation between idx_workclass and income is: 0.13693664382909343\n",
      "The correlation between idx_education and income is: 0.04611569765633624\n",
      "The correlation between idx_marital_status and income is: -0.3133594943582077\n",
      "The correlation between idx_occupation and income is: -0.1829482090625016\n",
      "The correlation between idx_relationship and income is: -0.2552405895314646\n",
      "The correlation between idx_race and income is: -0.0679025544900534\n",
      "The correlation between idx_sex and income is: -0.21669868107558524\n",
      "The correlation between idx_native_country and income is: -0.019815557146235275\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "feature = 14\n",
       "columnNames = Array(age, fnlwgt, education_num, capital_gain, capital_loss, hours_per_week, idx_workclass, idx_education, idx_marital_status, idx_occupation, idx_relationship, idx_race, idx_sex, idx_native_country, label)\n",
       "numFeature = 14\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "14"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.mllib.stat.Statistics\n",
    "\n",
    "var feature = 0;\n",
    "val columnNames = trainData_newDf.columns\n",
    "val numFeature = columnNames.size - 1\n",
    "while(feature < numFeature)\n",
    "{\n",
    "    var name = columnNames(feature)\n",
    "    var correlation = trainData_newDf.stat.corr(\"label\", name)\n",
    "    println(s\"The correlation between $name and income is: \" + correlation)\n",
    "    feature +=  1\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result calculated here are same with heat map in dataVisualization.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. assemble raw features "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- age: integer (nullable = true)\n",
      " |-- fnlwgt: double (nullable = true)\n",
      " |-- education_num: double (nullable = true)\n",
      " |-- capital_gain: double (nullable = true)\n",
      " |-- capital_loss: double (nullable = true)\n",
      " |-- hours_per_week: double (nullable = true)\n",
      " |-- idx_workclass: double (nullable = true)\n",
      " |-- idx_education: double (nullable = true)\n",
      " |-- idx_marital_status: double (nullable = true)\n",
      " |-- idx_occupation: double (nullable = true)\n",
      " |-- idx_relationship: double (nullable = true)\n",
      " |-- idx_race: double (nullable = true)\n",
      " |-- idx_sex: double (nullable = true)\n",
      " |-- idx_native_country: double (nullable = true)\n",
      " |-- label: double (nullable = true)\n",
      " |-- rawFeatures: vector (nullable = true)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "featureCol_train = Array(age, fnlwgt, education_num, capital_gain, capital_loss, hours_per_week, idx_workclass, idx_education, idx_marital_status, idx_occupation, idx_relationship, idx_race, idx_sex, idx_native_country)\n",
       "assembler_train = vecAssembler_ed78e22d83a1\n",
       "featured_trainData_df = [age: int, fnlwgt: double ... 14 more fields]\n",
       "featureCol_test = Array(age, fnlwgt, education_num, capital_gain, capital_loss, hours_per_week, idx_workclass, idx_education, idx_marital_status, idx_occupation, idx_relationship, idx_race, idx_sex, idx_native_country)\n",
       "assembler_test = vecAssembler_666dafc49bee\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "featured_testData_df: org.apache.spark.sql.DataFr...\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "vecAssembler_666dafc49bee"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//train\n",
    "val featureCol_train: Array[String] = trainData_newDf.columns.filter(_ != \"label\")\n",
    "val assembler_train = new VectorAssembler()\n",
    "                      .setInputCols(featureCol_train)\n",
    "                      .setOutputCol(\"rawFeatures\")\n",
    "val featured_trainData_df = assembler_train.transform(trainData_newDf)\n",
    "featured_trainData_df.printSchema\n",
    "//featured_trainData_df.show(1)\n",
    "//test\n",
    "val featureCol_test: Array[String] = testData_newDf.columns.filter(_ != \"label\")\n",
    "val assembler_test = new VectorAssembler()\n",
    "                      .setInputCols(featureCol_test)\n",
    "                      .setOutputCol(\"rawFeatures\")\n",
    "val featured_testData_df = assembler_test.transform(testData_newDf)\n",
    "//featured_testData_df.show(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. Prediction based on different machine learning algorithms:\n",
    "\n",
    "> 8.1 Logistic Regression\n",
    "\n",
    "> 8.2 Decision Tree\n",
    "\n",
    "> 8.3 Random Forest\n",
    "\n",
    "> 8.4 Naive Bayes\n",
    "\n",
    "> 8.5 Neural Network \n",
    "\n",
    "> 8.6 GBTClassifier\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.1 Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False positive rate = 0.05457746478873239\n",
      "False negative rate = 0.5218918918918919\n",
      "specificity for logistic model = 0.9454225352112676\n",
      "sensitivity for logistic model = 0.4781081081081081\n",
      "Confusion matrix for logistics model:\n",
      "10740.0  620.0   \n",
      "1931.0   1769.0  \n",
      "Accuracy for logistics model:\n",
      "0.8306108897742364\n",
      "Test Error for logistics model = 0.16938911022576364\n",
      "It took 1840 ms.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "stages = ArrayBuffer(RFormula(label ~ age + fnlwgt + education_num + capital_gain + capital_loss + hours_per_week + idx_workclass + idx_education + idx_marital_status + idx_occupation + idx_relationship + idx_race + idx_sex + idx_native_country) (uid=rFormula_9768e00f9ce9), logreg_cf54405bb8f4)\n",
       "pipeline = pipeline_15783c88f47f\n",
       "start_log = 1513754758418\n",
       "pipelineModel = pipeline_15783c88f47f\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "model_log: org.apache.spark.ml.c...\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "pipeline_15783c88f47f"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//Train a Logistic Model\n",
    "import scala.collection.mutable\n",
    "import org.apache.spark.ml.feature.RFormula\n",
    "import org.apache.spark.ml.{Pipeline, PipelineStage}\n",
    "import org.apache.spark.ml.classification.{LogisticRegression, LogisticRegressionModel, OneVsRest} \n",
    "\n",
    "val stages = new mutable.ArrayBuffer[PipelineStage]()\n",
    "stages += new RFormula().setFormula(\"label ~ age + fnlwgt + education_num + capital_gain + capital_loss + hours_per_week + idx_workclass + idx_education + idx_marital_status + idx_occupation + idx_relationship + idx_race + idx_sex + idx_native_country\")\n",
    "stages += new LogisticRegression()\n",
    "                    .setLabelCol(\"label\")\n",
    "                    .setFeaturesCol(\"rawFeatures\") \n",
    "                    .setMaxIter(10)\n",
    "                    .setTol(1E-6)\n",
    "                    .setFitIntercept(true)\n",
    "val pipeline = new Pipeline().setStages(stages.toArray)\n",
    "\n",
    "var start_log=System.currentTimeMillis()\n",
    "\n",
    "val pipelineModel = pipeline.fit(featured_trainData_df) \n",
    "val model_log = pipelineModel.stages.last.asInstanceOf[LogisticRegressionModel]\n",
    "//test the model\n",
    "val examFormular = new RFormula().setFormula(\"label ~ age + fnlwgt + education_num + capital_gain + capital_loss + hours_per_week + idx_workclass + idx_education + idx_marital_status + idx_occupation + idx_relationship + idx_race + idx_sex + idx_native_country\")\n",
    "val fittedRF = examFormular.fit(featured_testData_df)\n",
    "\n",
    "var end_log=System.currentTimeMillis()\n",
    "    \n",
    "    \n",
    "val preparedDF = fittedRF.transform(featured_testData_df)\n",
    "val prediction_log = model_log.transform(preparedDF)\n",
    "\n",
    "val predictionAndLabels_log = prediction_log.select(\"prediction\", \"label\").as[(Double,Double)].rdd\n",
    "val metrics_log = new MulticlassMetrics(predictionAndLabels_log)\n",
    "var confusion_log = metrics_log.confusionMatrix\n",
    "\n",
    "var TP_log = confusion_log(1,1)\n",
    "var TN_log = confusion_log(0,0)\n",
    "var FP_log = confusion_log(0,1)\n",
    "var FN_log = confusion_log(1,0)\n",
    "val total = TP_log + TN_log + FP_log + FN_log\n",
    "var mis_log = (FP_log+FN_log)/total\n",
    "var FPrate_log = FP_log/(TN_log + FP_log) \n",
    "var FNrate_log = FN_log/(FN_log + TP_log)\n",
    "var sensitivity_log = TP_log/(FN_log + TP_log)\n",
    "var specificity_log = TN_log/(TN_log + FP_log)\n",
    "println(\"False positive rate = \" + FPrate_log)\n",
    "println(\"False negative rate = \" + FNrate_log)\n",
    "println(\"specificity for logistic model = \" + specificity_log)\n",
    "println(\"sensitivity for logistic model = \" + sensitivity_log)\n",
    "\n",
    "var time_log=end_log-start_log\n",
    "\n",
    "println(\"Confusion matrix for logistics model:\")\n",
    "println(metrics_log.confusionMatrix)\n",
    "println(\"Accuracy for logistics model:\")\n",
    "println(metrics_log.accuracy)\n",
    "println(\"Test Error for logistics model = \" + (1.0 - metrics_log.accuracy))\n",
    "printf(\"It took %d ms.\\n\",time_log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix for one VS rest model:\n",
      "10740.0  620.0   \n",
      "1931.0   1769.0  \n",
      "Accuracy for one VS rest model:\n",
      "0.8306108897742364\n",
      "Test Error for one VS rest model = 0.16938911022576364\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "classifier = logreg_136956e95035\n",
       "ovr = oneVsRest_45023ae0a083\n",
       "model_ovr = oneVsRest_45023ae0a083\n",
       "predictions_ovr = [age: int, fnlwgt: double ... 15 more fields]\n",
       "predictionAndLabels_ovr = MapPartitionsRDD[368] at rdd at <console>:113\n",
       "metrics_ovr = org.apache.spark.mllib.evaluation.MulticlassMetrics@6b01d16f\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "org.apache.spark.mllib.evaluation.MulticlassMetrics@6b01d16f"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//the One vs Rest Classifier algorithm\n",
    "// instantiate the base classifier\n",
    "val classifier = new LogisticRegression()\n",
    "  .setMaxIter(10)\n",
    "  .setTol(1E-6)\n",
    "  .setFitIntercept(true)\n",
    "// instantiate the One Vs Rest Classifier.\n",
    "val ovr = new OneVsRest().setClassifier(classifier)\n",
    "                         .setLabelCol(\"label\")\n",
    "                         .setFeaturesCol(\"rawFeatures\") \n",
    "\n",
    "// train the multiclass model.\n",
    "val model_ovr = ovr.fit(featured_trainData_df)\n",
    "\n",
    "// evaluate the model \n",
    "val predictions_ovr = model_ovr.transform(featured_testData_df)\n",
    "\n",
    "val predictionAndLabels_ovr = predictions_ovr.select(\"prediction\", \"label\").as[(Double,Double)].rdd\n",
    "val metrics_ovr = new MulticlassMetrics(predictionAndLabels_ovr)\n",
    "\n",
    "println(\"Confusion matrix for one VS rest model:\")\n",
    "println(metrics_ovr.confusionMatrix)\n",
    "println(\"Accuracy for one VS rest model:\")\n",
    "println(metrics_ovr.accuracy)\n",
    "println(\"Test Error for one VS rest model = \" + (1.0 - metrics_ovr.accuracy))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.2 Decision Tree\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False positive rate = 0.04665492957746479\n",
      "False negative rate = 0.5094594594594595\n",
      "specificity for decision tree model = 0.9533450704225352\n",
      "sensitivity for decision tree model = 0.4905405405405405\n",
      "Confusion matrix for decision tree model:\n",
      "10830.0  530.0   \n",
      "1885.0   1815.0  \n",
      "Accuracy for decision tree model:\n",
      "0.8396414342629482\n",
      "Test Error for decision tree model = 0.16035856573705176\n",
      "It took 14925 ms.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "evaluator = mcEval_b27de2dc0018\n",
       "nFolds = 3\n",
       "paramGrid = \n",
       "dt = dtr_dc1d606bb831\n",
       "cv_dt = cv_cd5f4184d645\n",
       "start_dt = 1513917282663\n",
       "model_dt = cv_cd5f4184d645\n",
       "prediction_dt = [age: i...\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Array({\n",
       "})\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[age: int, fnlwgt: double ... 16 more fields]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//fits the decision tree model and evaluate the model on test data\n",
    "import org.apache.spark.ml.evaluation.MulticlassClassificationEvaluator\n",
    "import org.apache.spark.ml.regression.DecisionTreeRegressionModel\n",
    "import org.apache.spark.ml.regression.DecisionTreeRegressor\n",
    "import org.apache.spark.ml.tuning.{ParamGridBuilder, CrossValidator}\n",
    "import org.apache.spark.ml.feature.Binarizer\n",
    "\n",
    "val evaluator = new MulticlassClassificationEvaluator()\n",
    "                          .setLabelCol(\"label\")\n",
    "                          .setPredictionCol(\"prediction\")\n",
    "val nFolds = 3\n",
    "val paramGrid = new ParamGridBuilder().build() \n",
    "\n",
    "val dt = new DecisionTreeRegressor().setLabelCol(\"label\").setFeaturesCol(\"rawFeatures\").setMaxBins(50)\n",
    "\n",
    "val cv_dt = new CrossValidator()\n",
    "  .setEstimator(dt)\n",
    "  .setEvaluator(evaluator) \n",
    "  .setEstimatorParamMaps(paramGrid)\n",
    "  .setNumFolds(nFolds)\n",
    "\n",
    "var start_dt=System.currentTimeMillis()\n",
    "val model_dt = cv_dt.fit(featured_trainData_df)\n",
    "//evaluate\n",
    "var prediction_dt = model_dt.transform(featured_testData_df)\n",
    "var end_dt=System.currentTimeMillis()\n",
    "    \n",
    "//binarize the predition \n",
    "val binarizer: Binarizer = new Binarizer().\n",
    "  setInputCol(\"prediction\").\n",
    "  setOutputCol(\"binarized_pred\").\n",
    "  setThreshold(0.5)\n",
    "\n",
    "prediction_dt = binarizer.transform(prediction_dt) \n",
    "val predictionAndLabels_dt = prediction_dt.select(\"binarized_pred\", \"label\").as[(Double,Double)].rdd\n",
    "val metrics_dt = new MulticlassMetrics(predictionAndLabels_dt)\n",
    "val time_dt = end_dt - start_dt\n",
    "\n",
    "var confusion_dt = metrics_dt.confusionMatrix\n",
    "\n",
    "var TP_dt = confusion_dt(1,1)\n",
    "var TN_dt = confusion_dt(0,0)\n",
    "var FP_dt = confusion_dt(0,1)\n",
    "var FN_dt = confusion_dt(1,0)\n",
    "//val total = TP_log + TN_log + FP_log + FN_log\n",
    "var mis_dt = (FP_dt+FN_dt)/total\n",
    "var FPrate_dt = FP_dt/(TN_dt + FP_dt) \n",
    "var FNrate_dt = FN_dt/(FN_dt + TP_dt)\n",
    "var sensitivity_dt = TP_dt/(FN_dt + TP_dt)\n",
    "var specificity_dt = TN_dt/(TN_dt + FP_dt)\n",
    "println(\"False positive rate = \" + FPrate_dt)\n",
    "println(\"False negative rate = \" + FNrate_dt)\n",
    "println(\"specificity for decision tree model = \" + specificity_dt)\n",
    "println(\"sensitivity for decision tree model = \" + sensitivity_dt)\n",
    "\n",
    "println(\"Confusion matrix for decision tree model:\")\n",
    "println(metrics_dt.confusionMatrix)\n",
    "println(\"Accuracy for decision tree model:\")\n",
    "println(metrics_dt.accuracy)\n",
    "println(\"Test Error for decision tree model = \" + (1.0 - metrics_dt.accuracy))\n",
    "printf(\"It took %d ms.\\n\",time_dt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.3 Random Forest\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+-------------+------------+------------+--------------+-----+-------------+-------------+------------------+--------------+----------------+--------+-------+------------------+--------------------+--------------------+--------------------+----------+\n",
      "|age|  fnlwgt|education_num|capital_gain|capital_loss|hours_per_week|label|idx_workclass|idx_education|idx_marital_status|idx_occupation|idx_relationship|idx_race|idx_sex|idx_native_country|         rawFeatures|       rawPrediction|         probability|prediction|\n",
      "+---+--------+-------------+------------+------------+--------------+-----+-------------+-------------+------------------+--------------+----------------+--------+-------+------------------+--------------------+--------------------+--------------------+----------+\n",
      "| 25|226802.0|          7.0|         0.0|         0.0|          40.0|  0.0|          0.0|          5.0|               1.0|           6.0|             2.0|     1.0|    0.0|               0.0|[25.0,226802.0,7....|[9.85935379363931...|[0.98593537936393...|       0.0|\n",
      "| 38| 89814.0|          9.0|         0.0|         0.0|          50.0|  0.0|          0.0|          0.0|               0.0|           9.0|             0.0|     0.0|    0.0|               0.0|(14,[0,1,2,5,9],[...|[6.58794415569924...|[0.65879441556992...|       0.0|\n",
      "| 28|336951.0|         12.0|         0.0|         0.0|          40.0|  1.0|          2.0|          6.0|               0.0|          11.0|             0.0|     0.0|    0.0|               0.0|(14,[0,1,2,5,6,7,...|[6.45935594326883...|[0.64593559432688...|       0.0|\n",
      "+---+--------+-------------+------------+------------+--------------+-----+-------------+-------------+------------------+--------------+----------------+--------+-------+------------------+--------------------+--------------------+--------------------+----------+\n",
      "only showing top 3 rows\n",
      "\n",
      "Confusion matrix for random forest:\n",
      "10788.0  572.0   \n",
      "1727.0   1973.0  \n",
      "Accuracy for random forest:\n",
      "0.84734395750332\n",
      "Test Error for random forest = 0.15265604249667997\n",
      "False positive rate = 0.05035211267605634\n",
      "False negative rate = 0.46675675675675676\n",
      "specificity = 0.9496478873239437\n",
      "sensitivity = 0.5332432432432432\n",
      "It took 12754 ms.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "rf = rfc_dcfa881eddef\n",
       "cv_rf = cv_b36151ce33ef\n",
       "start_rf = 1513917396827\n",
       "model_rf = cv_b36151ce33ef\n",
       "prediction_rf = [age: int, fnlwgt: double ... 17 more fields]\n",
       "end_rf = 1513917409581\n",
       "predictionAndLabels_rf = MapPartitionsRDD[594] at rdd at <console>:112\n",
       "metrics_rf = org.apache.spark.mllib.evaluation.MulticlassMetrics@1070ce42\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "confusion: org.apache.spark.mllib.linalg.Matrix...\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "org.apache.spark.mllib.evaluation.MulticlassMetrics@1070ce42"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.ml.evaluation.MulticlassClassificationEvaluator\n",
    "import org.apache.spark.mllib.evaluation.MulticlassMetrics\n",
    "//Train a RandomForest Model\n",
    "val rf = new RandomForestClassifier()\n",
    "                  .setLabelCol(\"label\")\n",
    "                  .setFeaturesCol(\"rawFeatures\")\n",
    "                  .setNumTrees(10)\n",
    "                  .setMaxBins(50)\n",
    "\n",
    "val cv_rf = new CrossValidator()\n",
    "  .setEstimator(rf)\n",
    "  .setEvaluator(evaluator) \n",
    "  .setEstimatorParamMaps(paramGrid)\n",
    "  .setNumFolds(nFolds)\n",
    "\n",
    "var start_rf=System.currentTimeMillis()\n",
    "\n",
    "val model_rf = cv_rf.fit(featured_trainData_df)\n",
    "//evaluate random forest model\n",
    "val prediction_rf = model_rf.transform(featured_testData_df)\n",
    "var end_rf=System.currentTimeMillis()\n",
    "\n",
    "prediction_rf.show(3)\n",
    "val predictionAndLabels_rf = prediction_rf.select(\"prediction\", \"label\").as[(Double,Double)].rdd\n",
    "val metrics_rf = new MulticlassMetrics(predictionAndLabels_rf)\n",
    "var confusion = metrics_rf.confusionMatrix\n",
    "val time_rf = end_rf - start_rf\n",
    "\n",
    "println(\"Confusion matrix for random forest:\")\n",
    "println(confusion)\n",
    "println(\"Accuracy for random forest:\")\n",
    "println(metrics_rf.accuracy)\n",
    "println(\"Test Error for random forest = \" + (1.0 - metrics_rf.accuracy))\n",
    "\n",
    "var TP = confusion(1,1)\n",
    "var TN = confusion(0,0)\n",
    "var FP = confusion(0,1)\n",
    "var FN = confusion(1,0)\n",
    "val total = TP + TN + FP + FN\n",
    "var mis = (FP+FN)/total\n",
    "var FPrate = FP/(TN + FP) \n",
    "var FNrate = FN/(FN + TP)\n",
    "var sensitivity = TP/(FN + TP)\n",
    "var specificity = TN/(TN + FP)\n",
    "println(\"False positive rate = \" + FPrate)\n",
    "println(\"False negative rate = \" + FNrate)\n",
    "println(\"specificity = \" + specificity)\n",
    "println(\"sensitivity = \" + sensitivity)\n",
    "printf(\"It took %d ms.\\n\",time_rf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.4 Naive Bayes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False positive rate = 0.045158450704225354\n",
      "False negative rate = 0.7683783783783784\n",
      "specificity for naive bayes model = 0.9548415492957747\n",
      "sensitivity for naive bayes model = 0.23162162162162162\n",
      "Confusion matrix for Naive Bayes:\n",
      "10847.0  513.0  \n",
      "2843.0   857.0  \n",
      "Accuracy for Naive Bayes:\n",
      "0.7771580345285525\n",
      "Test Error for Naive Bayes = 0.22284196547144752\n",
      "It took 5616 ms.\n",
      "+---+-------+-------------+------------+------------+--------------+-------------+-------------+------------------+--------------+----------------+--------+-------+------------------+-----+--------------------+\n",
      "|age| fnlwgt|education_num|capital_gain|capital_loss|hours_per_week|idx_workclass|idx_education|idx_marital_status|idx_occupation|idx_relationship|idx_race|idx_sex|idx_native_country|label|         rawFeatures|\n",
      "+---+-------+-------------+------------+------------+--------------+-------------+-------------+------------------+--------------+----------------+--------+-------+------------------+-----+--------------------+\n",
      "| 39|77516.0|         13.0|      2174.0|         0.0|          40.0|          3.0|          2.0|               1.0|           3.0|             1.0|     0.0|    0.0|               0.0|  0.0|[39.0,77516.0,13....|\n",
      "| 28|83311.0|         13.0|         0.0|         0.0|          13.0|          1.0|          2.0|               0.0|           2.0|             0.0|     0.0|    0.0|               0.0|  0.0|(14,[0,1,2,5,6,7,...|\n",
      "+---+-------+-------------+------------+------------+--------------+-------------+-------------+------------------+--------------+----------------+--------+-------+------------------+-----+--------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "nb = nb_e95456f172ba\n",
       "cv_nb = cv_06afee0e1292\n",
       "start_nb = 1513917446655\n",
       "model_nb = cv_06afee0e1292\n",
       "predictions_nb = [age: int, fnlwgt: double ... 17 more fields]\n",
       "end_nb = 1513917452271\n",
       "predictionAndLabels_nb = MapPartitionsRDD[719] at rdd at <console>:122\n",
       "metrics_nb = org.apache.spark.mllib.evaluation.MulticlassMetrics@7bebd76a\n",
       "time_nb = 5616\n",
       "confusion_nb = \n",
       "TP_nb = 857.0\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TN_nb: Doub...\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "857.0"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.ml.classification.NaiveBayes\n",
    "\n",
    "// Train a NaiveBayes model.\n",
    "\n",
    "\n",
    "val nb = new NaiveBayes()                      \n",
    "          .setLabelCol(\"label\")\n",
    "          .setFeaturesCol(\"rawFeatures\") \n",
    "\n",
    "val cv_nb = new CrossValidator()\n",
    "  .setEstimator(nb)\n",
    "  .setEvaluator(evaluator) \n",
    "  .setEstimatorParamMaps(paramGrid)\n",
    "  .setNumFolds(nFolds)\n",
    "\n",
    "var start_nb = System.currentTimeMillis()\n",
    "val model_nb = cv_nb.fit(featured_trainData_df)\n",
    "\n",
    "val predictions_nb = model_nb.transform(featured_testData_df)\n",
    "var end_nb = System.currentTimeMillis()\n",
    "\n",
    "val predictionAndLabels_nb = predictions_nb.select(\"prediction\", \"label\").as[(Double,Double)].rdd\n",
    "val metrics_nb = new MulticlassMetrics(predictionAndLabels_nb)\n",
    "val time_nb = end_nb - start_nb\n",
    "\n",
    "var confusion_nb = metrics_nb.confusionMatrix\n",
    "\n",
    "var TP_nb = confusion_nb(1,1)\n",
    "var TN_nb = confusion_nb(0,0)\n",
    "var FP_nb = confusion_nb(0,1)\n",
    "var FN_nb = confusion_nb(1,0)\n",
    "//val total = TP_log + TN_log + FP_log + FN_log\n",
    "var mis_nb = (FP_nb+FN_nb)/total\n",
    "var FPrate_nb = FP_nb/(TN_nb + FP_nb) \n",
    "var FNrate_nb = FN_nb/(FN_nb + TP_nb)\n",
    "var sensitivity_nb = TP_nb/(FN_nb + TP_nb)\n",
    "var specificity_nb = TN_nb/(TN_nb + FP_nb)\n",
    "println(\"False positive rate = \" + FPrate_nb)\n",
    "println(\"False negative rate = \" + FNrate_nb)\n",
    "println(\"specificity for naive bayes model = \" + specificity_nb)\n",
    "println(\"sensitivity for naive bayes model = \" + sensitivity_nb)\n",
    "\n",
    "\n",
    "println(\"Confusion matrix for Naive Bayes:\")\n",
    "println(metrics_nb.confusionMatrix)\n",
    "println(\"Accuracy for Naive Bayes:\")\n",
    "println(metrics_nb.accuracy)\n",
    "println(\"Test Error for Naive Bayes = \" + (1.0 - metrics_nb.accuracy))\n",
    "printf(\"It took %d ms.\\n\",time_nb)\n",
    "\n",
    "featured_trainData_df.show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.5 Neural Network \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "layers1 = Array(14, 7, 4, 3)\n",
       "layers2 = Array(14, 28, 10, 3)\n",
       "layers3 = Array(14, 40, 28, 14, 7, 3)\n",
       "start_nn = 1513917512253\n",
       "trainer_nn_1 = mlpc_638150417509\n",
       "cv_nn_1 = cv_c6211d15f71b\n",
       "model_nn_1 = cv_c6211d15f71b\n",
       "prediction_nn_1 = [age: int, fnlwgt: double ... 15 more fields]\n",
       "end_nn = 1513917598132\n",
       "time_nn = 85879\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "85879"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.ml.classification.MultilayerPerceptronClassifier\n",
    "import org.apache.spark.ml.evaluation.MulticlassClassificationEvaluator\n",
    "// try three different layers  \n",
    "val layers1 = Array[Int](14, 7, 4, 3)\n",
    "val layers2 = Array[Int](14, 28, 10, 3)\n",
    "val layers3 = Array[Int](14, 40, 28, 14, 7, 3)\n",
    "val start_nn = System.currentTimeMillis()\n",
    "// create the trainer and set its parameters\n",
    "val trainer_nn_1 = new MultilayerPerceptronClassifier().\n",
    "  setLabelCol(\"label\").\n",
    "  setFeaturesCol(\"rawFeatures\").\n",
    "  setLayers(layers1).\n",
    "  setBlockSize(128).\n",
    "  setSeed(1234L).\n",
    "  setMaxIter(100)\n",
    "// train the model and predict\n",
    "val cv_nn_1 = new CrossValidator()\n",
    "  .setEstimator(trainer_nn_1)\n",
    "  .setEvaluator(evaluator) \n",
    "  .setEstimatorParamMaps(paramGrid)\n",
    "  .setNumFolds(nFolds)\n",
    "\n",
    "val model_nn_1 = cv_nn_1.fit(featured_trainData_df)\n",
    "val prediction_nn_1= model_nn_1.transform(featured_testData_df)\n",
    "val end_nn = System.currentTimeMillis()\n",
    "val time_nn = end_nn - start_nn\n",
    "\n",
    "/*other two nn with diff parameters \n",
    "val trainer_nn_2=new MultilayerPerceptronClassifier().\n",
    "  setLabelCol(\"label\").\n",
    "  setFeaturesCol(\"rawFeatures\").\n",
    "  setLayers(layers2).\n",
    "  setBlockSize(128).\n",
    "  setSeed(1234L).\n",
    "  setMaxIter(500)\n",
    "val model_nn_2 = trainer_nn_2.fit(featured_trainData_df)\n",
    "val prediction_nn_2= model_nn_2.transform(featured_testData_df)\n",
    "\n",
    "\n",
    "val trainer_nn_3=new MultilayerPerceptronClassifier().\n",
    "  setLabelCol(\"label\").\n",
    "  setFeaturesCol(\"rawFeatures\").\n",
    "  setLayers(layers3).\n",
    "  setBlockSize(128).\n",
    "  setSeed(1234L).\n",
    "  setMaxIter(100)\n",
    "val model_nn_3 = trainer_nn_3.fit(featured_trainData_df)\n",
    "val prediction_nn_3= model_nn_3.transform(featured_testData_df)\n",
    "*/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False positive rate = 0.007394366197183098\n",
      "False negative rate = 0.8248648648648649\n",
      "specificity for neural network model = 0.9926056338028169\n",
      "sensitivity for neural network model = 0.17513513513513512\n",
      "Test for nn_1:\n",
      "Confusion matrix\n",
      "11276.0  84.0   \n",
      "3052.0   648.0  \n",
      "Accuracy=0.7917662682602922\n",
      "Test Error = 0.20823373173970783\n",
      "It took 85879 ms.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "spark = org.apache.spark.sql.SparkSession@6481ce0d\n",
       "predictionAndLabels_nn1 = MapPartitionsRDD[1420] at rdd at <console>:129\n",
       "metrics_nn_1 = org.apache.spark.mllib.evaluation.MulticlassMetrics@5f7f23ce\n",
       "confusion_nn = \n",
       "TP_nn = 648.0\n",
       "TN_nn = 11276.0\n",
       "FP_nn = 84.0\n",
       "FN_nn = 3052.0\n",
       "mis_nn = 0.22284196547144755\n",
       "FPrate_nn = 0.007394366197183098\n",
       "FNrate_nn = 0.8248648648648649\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "sensitivity_nn: Doub...\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0.8248648648648649"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.ml.evaluation.MulticlassClassificationEvaluator\n",
    "import org.apache.spark.mllib.evaluation.MulticlassMetrics\n",
    "val spark = SparkSession.builder().appName(\"Spark-ML\").master(\"local\").getOrCreate()\n",
    "import spark.implicits._\n",
    "\n",
    "// compute accuracy on the test set\n",
    "var predictionAndLabels_nn1 = prediction_nn_1.select(\"prediction\", \"label\").as[(Double,Double)].rdd\n",
    "val metrics_nn_1 = new MulticlassMetrics(predictionAndLabels_nn1)\n",
    "\n",
    "var confusion_nn = metrics_nn_1.confusionMatrix\n",
    "\n",
    "var TP_nn = confusion_nn(1,1)\n",
    "var TN_nn = confusion_nn(0,0)\n",
    "var FP_nn = confusion_nn(0,1)\n",
    "var FN_nn = confusion_nn(1,0)\n",
    "//val total = TP_log + TN_log + FP_log + FN_log\n",
    "var mis_nn = (FP_nb+FN_nb)/total\n",
    "var FPrate_nn = FP_nn/(TN_nn + FP_nn) \n",
    "var FNrate_nn = FN_nn/(FN_nn + TP_nn)\n",
    "var sensitivity_nn = TP_nn/(FN_nn + TP_nn)\n",
    "var specificity_nn = TN_nn/(TN_nn + FP_nn)\n",
    "println(\"False positive rate = \" + FPrate_nn)\n",
    "println(\"False negative rate = \" + FNrate_nn)\n",
    "println(\"specificity for neural network model = \" + specificity_nn)\n",
    "println(\"sensitivity for neural network model = \" + sensitivity_nn)\n",
    "\n",
    "println(\"Test for nn_1:\")\n",
    "println(\"Confusion matrix\")\n",
    "println(metrics_nn_1.confusionMatrix)\n",
    "println(\"Accuracy=\"+metrics_nn_1.accuracy)\n",
    "println(\"Test Error = \" + (1.0 - metrics_nn_1.accuracy))\n",
    "printf(\"It took %d ms.\\n\",time_nn)\n",
    "\n",
    "/* other model with diff parameters\n",
    "var predictionAndLabels_nn2 = prediction_nn_2.select(\"prediction\", \"label\").as[(Double,Double)].rdd\n",
    "val metrics_nn_2 = new MulticlassMetrics(predictionAndLabels_nn2)\n",
    "println(\"Test for nn_2:\")\n",
    "println(\"Confusion matrix\")\n",
    "println(metrics_nn_2.confusionMatrix)\n",
    "println(\"Accuracy=\"+metrics_nn_2.accuracy)\n",
    "println(\"Test Error = \" + (1.0 - metrics_nn_2.accuracy))\n",
    "var predictionAndLabels_nn3 = prediction_nn_3.select(\"prediction\", \"label\").as[(Double,Double)].rdd\n",
    "val metrics_nn_3 = new MulticlassMetrics(predictionAndLabels_nn3)\n",
    "println(\"Test for nn_3:\")\n",
    "println(\"Confusion matrix\")\n",
    "println(metrics_nn_3.confusionMatrix)\n",
    "println(\"Accuracy=\"+metrics_nn_3.accuracy)\n",
    "println(\"Test Error = \" + (1.0 - metrics_nn_3.accuracy))\n",
    "*/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.6 Gradient-Boosted Trees Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False positive rate = 0.06778169014084508\n",
      "False negative rate = 0.3816216216216216\n",
      "specificity for GBT classifier model = 0.9322183098591549\n",
      "sensitivity for GBT classifier model = 0.6183783783783784\n",
      "Test for gbt_1:\n",
      "Confusion matrix\n",
      "10590.0  770.0   \n",
      "1412.0   2288.0  \n",
      "Accuracy=0.8551128818061089\n",
      "Test Error = 0.1448871181938911\n",
      "It took 32249 ms.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "trainer_gbt_1 = gbtc_b3728421fb21\n",
       "cv_gbt_1 = cv_e3b2dc149575\n",
       "start_gbt = 1513917665137\n",
       "model_gbt_1 = cv_e3b2dc149575\n",
       "prediction_gbt_1 = [age: int, fnlwgt: double ... 17 more fields]\n",
       "end_gbt = 1513917697386\n",
       "predictionAndLabels = MapPartitionsRDD[2592] at rdd at <console>:130\n",
       "metrics_gbt_1 = org.apache.spark.mllib.evaluation.MulticlassMetrics@5eb2540d\n",
       "time_gbt = 32249\n",
       "confusion_gbt = \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "1412.0   2288.0...\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "10590.0  770.0   \n",
       "1412.0   2288.0  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.ml.classification.GBTClassifier\n",
    "val trainer_gbt_1 = new GBTClassifier().\n",
    "  setLabelCol(\"label\").\n",
    "  setFeaturesCol(\"rawFeatures\").\n",
    "  setMaxIter(10).\n",
    "  setMaxBins(50)\n",
    "\n",
    "val cv_gbt_1 = new CrossValidator()\n",
    "  .setEstimator(trainer_gbt_1)\n",
    "  .setEvaluator(evaluator) \n",
    "  .setEstimatorParamMaps(paramGrid)\n",
    "  .setNumFolds(nFolds)\n",
    "\n",
    "val start_gbt = System.currentTimeMillis()\n",
    "val model_gbt_1 = cv_gbt_1.fit(featured_trainData_df)\n",
    "// compute accuracy on the test set\n",
    "val prediction_gbt_1= model_gbt_1.transform(featured_testData_df)\n",
    "val end_gbt = System.currentTimeMillis()\n",
    "\n",
    "var predictionAndLabels = prediction_gbt_1.select(\"prediction\", \"label\").as[(Double,Double)].rdd\n",
    "val metrics_gbt_1 = new MulticlassMetrics(predictionAndLabels)\n",
    "val time_gbt = end_gbt - start_gbt\n",
    "\n",
    "var confusion_gbt = metrics_gbt_1.confusionMatrix\n",
    "\n",
    "var TP_gbt = confusion_gbt(1,1)\n",
    "var TN_gbt = confusion_gbt(0,0)\n",
    "var FP_gbt = confusion_gbt(0,1)\n",
    "var FN_gbt = confusion_gbt(1,0)\n",
    "//val total = TP_log + TN_log + FP_log + FN_log\n",
    "var mis_gbt = (FP_gbt+FN_gbt)/total\n",
    "var FPrate_gbt = FP_gbt/(TN_gbt + FP_gbt) \n",
    "var FNrate_gbt = FN_gbt/(FN_gbt + TP_gbt)\n",
    "var sensitivity_gbt = TP_gbt/(FN_gbt + TP_gbt)\n",
    "var specificity_gbt = TN_gbt/(TN_gbt + FP_gbt)\n",
    "println(\"False positive rate = \" + FPrate_gbt)\n",
    "println(\"False negative rate = \" + FNrate_gbt)\n",
    "println(\"specificity for GBT classifier model = \" + specificity_gbt)\n",
    "println(\"sensitivity for GBT classifier model = \" + sensitivity_gbt)\n",
    "\n",
    "println(\"Test for gbt_1:\")\n",
    "println(\"Confusion matrix\")\n",
    "println(metrics_gbt_1.confusionMatrix)\n",
    "println(\"Accuracy=\"+metrics_gbt_1.accuracy)\n",
    "println(\"Test Error = \" + (1.0 - metrics_gbt_1.accuracy))\n",
    "printf(\"It took %d ms.\\n\",time_gbt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# 9. Conclusion\n",
    "\n",
    "| item |logistic regression  | decision tree|random forest |NaiveBayes|Neural Network| GBT|\n",
    "|-------|----------------|-----------------|-----------------|-----------------|-------|---------|\n",
    "|Accuracy| 0.83061   |  0.83964       | 0.84734      |  0.77716| 0.79177   |  0.85511  |\n",
    "|Misclassification Rate| 0.16939   | 0.16036   |   0.15266|  0.22284  |  0.20823 |  0.14489 |\n",
    "|False Positive Rate |     0.05458       |   0.04665        |   0.05035| 0.04516  | 0.00739  |  0.06778 |\n",
    "|False Negative Rate| 0.52190  | 0.50946 |0.46676 |   0.76838  |  0.82486  |  0.38162   |\n",
    "|sensitivity (true pos)|  0.47811   |  0.49054   | 0.53324 | 0.23162   |    0.17514 | 0.61838   |\n",
    "|specificity (true neg) |  0.94542  |  0.95335  |   0.94965  |  0.95484   | 0.99261  |  0.93222    |\n",
    "|time| 1840ms | 2152ms  |6539ms | 1216ms|   83070ms |    17496ms |\n",
    "|time with cross validation| --| 14925ms | 12754ms  | 5616ms  | 85879ms  |  32249ms |\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "It is obvious that:\n",
    "> 1. Higher Accuracy, lower Misclassification rate. <br/>\n",
    "2. Naive Bayes took the least time, Neural Network took the most time.<br/>\n",
    "3. Naive Bayes procudes lowest overall accuracy, Gradient-Boosted Trees Classifier produces highest overall accuracy.<br/>\n",
    "4. Neural Network has the lowest False Positive rate and highest specificity which means Neural Network is best at predicting negative (if a person will get income <= 50k), Gradient-Boosted Trees Classifier has highest False Positive rate and lowest specificity which means is worst at prediction negative.<br/>\n",
    "5. Gradient-Boosted Trees Classifier has lowest False Negative rate and highest sensitivity which means GBT is the best model at predition positive (if a person will get income > 50k), Neural Network has the highest False Negative rate and lowest sensitivity which means Neural Network is the worst at prediction positive.<br/>\n",
    "\n",
    "As we can see from the above table, Gradient-Boosted Trees Classifier produces highest overall Accuracy, and compared to other models, it took less time, thus, it is the best model for our dataset. <br/>\n",
    "\n",
    "**Gradient boosting and Random Forest** are the most accurate model among the six, both of them are improved version of Decision treewhich coincide with dataVisualization.ipynb result. <br/>\n",
    "**Gradient boosting** have four improvemnts, Tree Constraints, Shrinkage, Random sampling and Penalized Learning, which make it more accurate.<br/>\n",
    "**Random Forest** operates by constructing a multitude of decision trees, based on Bootstrap Aggregation(bagging) on training samples. Thus, it is able to classify large amounts of data with accuracy.<br/>\n",
    "\n",
    "In data description reports, Naive Bayes model achieve 83.88% accuracy, however, our NB model in both spark and scikit learn only achieve 77%-79% accuracy. For prediction result of other algorithm, we got the error rate very close to what they got in data description webpage."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Apache Toree - Scala",
   "language": "scala",
   "name": "apache_toree_scala"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "2.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
